{"block_file": {"custom/mini_test.sql:custom:sql:mini test": {"content": "--SELECT table_schema, table_name\nFROM information_schema.tables\nWHERE table_schema = 'raw'\nORDER BY table_name;\n Docs: https://docs.mage.ai/guides/sql-blocks\n", "file_path": "custom/mini_test.sql", "language": "sql", "type": "custom", "uuid": "mini_test"}, "custom/test_pg_connection.py:custom:python:test pg connection": {"content": "from mage_ai.data_preparation.decorators import custom\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, profile)) as loader:\n        rows = loader.load(\"SELECT current_database() AS db, current_user AS usr;\")\n        print(\"DB/USER:\", rows)\n\n        exists = loader.load(\"SELECT to_regclass('raw.qb_invoices') AS tbl;\")\n        print(\"to_regclass:\", exists)\n\n        tables = loader.load(\"\"\"\n            SELECT table_schema, table_name\n            FROM information_schema.tables\n            WHERE table_schema = 'raw'\n            ORDER BY 1,2;\n        \"\"\")\n        print(\"tables in raw:\", tables)\n\n    return {\"ok\": True}\n\n\n", "file_path": "custom/test_pg_connection.py", "language": "python", "type": "custom", "uuid": "test_pg_connection"}, "custom/test_pg_connection_2.py:custom:python:test pg connection 2": {"content": "from mage_ai.data_preparation.decorators import custom\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\n\n@custom\ndef test_pg_connection_2(*args, **kwargs):\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, profile)) as loader:\n        info = loader.load(\"\"\"\n            SELECT current_database() AS db,\n                   current_schema()  AS schema,\n                   current_user      AS usr;\n        \"\"\")\n\n        exists = loader.load(\"\"\"\n            SELECT to_regclass('raw.qb_invoices') AS tbl;\n        \"\"\")\n\n        tables = loader.load(\"\"\"\n            SELECT table_schema, table_name\n            FROM information_schema.tables\n            WHERE table_schema = 'raw'\n            ORDER BY 1,2;\n        \"\"\")\n\n    print(\"DB INFO:\", info)\n    print(\"to_regclass raw.qb_invoices:\", exists)\n    print(\"tables in raw:\", tables)\n\n    return {\"ok\": True}\n", "file_path": "custom/test_pg_connection_2.py", "language": "python", "type": "custom", "uuid": "test_pg_connection_2"}, "data_exporters/upsert_invoices_to_postgres.py:data_exporter:python:upsert invoices to postgres": {"content": "import json\nfrom datetime import timezone\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef upsert_invoices_to_postgres(data, *args, **kwargs):\n    \"\"\"\n    Recibe output del transformer:\n      data = {\"invoices\": invoices_df, \"lines\": lines_df}\n    y hace upsert solo a raw.qb_invoices.\n    \"\"\"\n\n    if not data or \"invoices\" not in data or data[\"invoices\"] is None:\n        return {\"inserted_or_updated\": 0, \"note\": \"No invoices data returned from transformer.\"}\n\n    invoices_df = data[\"invoices\"]\n\n    # Si est\u00e1 vac\u00edo\n    if hasattr(invoices_df, \"empty\") and invoices_df.empty:\n        return {\"inserted_or_updated\": 0, \"note\": \"Invoices dataframe is empty.\"}\n\n    # DataFrame -> list[dict]\n    records = invoices_df.to_dict(\"records\")\n\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    profile = \"default\"\n\n    rows = []\n    for r in records:\n        # OJO: payload/request_payload puede ya venir como dict o como string\n        payload = r.get(\"payload\")\n        request_payload = r.get(\"request_payload\")\n\n        # Normalizamos fechas: si ya son string ISO, las dejamos;\n        # si son datetime, las pasamos a UTC ISO.\n        def to_utc_iso(x):\n            if x is None:\n                return None\n            if isinstance(x, str):\n                return x\n            # datetime\n            return x.astimezone(timezone.utc).isoformat()\n\n        rows.append({\n            \"id\": str(r.get(\"id\")),\n            \"payload\": json.dumps(payload) if isinstance(payload, (dict, list)) else (payload or \"{}\"),\n            \"ingested_at_utc\": to_utc_iso(r.get(\"ingested_at_utc\")),\n            \"extract_window_start_utc\": to_utc_iso(r.get(\"extract_window_start_utc\")),\n            \"extract_window_end_utc\": to_utc_iso(r.get(\"extract_window_end_utc\")),\n            \"page_number\": int(r.get(\"page_number\") or 0),\n            \"page_size\": int(r.get(\"page_size\") or 0),\n            \"request_payload\": json.dumps(request_payload) if isinstance(request_payload, (dict, list)) else (request_payload or \"{}\"),\n        })\n\n    sql = \"\"\"\n    INSERT INTO raw.qb_invoices (\n      id, payload, ingested_at_utc, extract_window_start_utc, extract_window_end_utc,\n      page_number, page_size, request_payload\n    )\n    VALUES (\n      %(id)s, %(payload)s::jsonb, %(ingested_at_utc)s::timestamptz,\n      %(extract_window_start_utc)s::timestamptz, %(extract_window_end_utc)s::timestamptz,\n      %(page_number)s, %(page_size)s, %(request_payload)s::jsonb\n    )\n    ON CONFLICT (id) DO UPDATE SET\n      payload = EXCLUDED.payload,\n      ingested_at_utc = EXCLUDED.ingested_at_utc,\n      extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n      extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n      page_number = EXCLUDED.page_number,\n      page_size = EXCLUDED.page_size,\n      request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    with Postgres.with_config(ConfigFileLoader(config_path, profile)) as loader:\n        for row in rows:\n            loader.execute(sql, **row)\n\n        # opcional: devuelve count real en DB\n        count = loader.load(\"SELECT COUNT(*) AS n FROM raw.qb_invoices;\")\n\n    return {\"inserted_or_updated\": len(rows), \"raw.qb_invoices_count\": count}\n", "file_path": "data_exporters/upsert_invoices_to_postgres.py", "language": "python", "type": "data_exporter", "uuid": "upsert_invoices_to_postgres"}, "data_loaders/customers_extract.py:data_loader:python:customers extract": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/customers_extract.py", "language": "python", "type": "data_loader", "uuid": "customers_extract"}, "data_loaders/data_loader_invoice.py:data_loader:python:data loader invoice": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/data_loader_invoice.py", "language": "python", "type": "data_loader", "uuid": "data_loader_invoice"}, "data_loaders/invoices_extract.py:data_loader:python:invoices extract": {"content": "import base64\nimport random\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Optional\n\n\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef iso_utc(dt: datetime) -> str:\n    return dt.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n\n\ndef parse_iso_z(s: str) -> datetime:\n    return datetime.fromisoformat(s.replace('Z', '+00:00')).astimezone(timezone.utc)\n\n\ndef iter_daily_windows(start_iso: str, end_iso: str):\n    start = parse_iso_z(start_iso)\n    end = parse_iso_z(end_iso)\n    cur = start\n    while cur < end:\n        nxt = min(cur + timedelta(days=1), end)\n        yield cur, nxt\n        cur = nxt\n\n\ndef get_access_token() -> str:\n    client_id = get_secret_value('qb_clientid')\n    client_secret = get_secret_value('qb_clientsecret')\n    refresh_token = get_secret_value('qb_refreshtoken')\n\n    basic = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        \"Authorization\": f\"Basic {basic}\",\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    r = requests.post(\n        \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\",\n        headers=headers,\n        data=data,\n        timeout=30,\n    )\n    r.raise_for_status()\n    return r.json()[\"access_token\"]\n\n\ndef qbo_base_url() -> str:\n    return \"https://sandbox-quickbooks.api.intuit.com\"\n\n\ndef qbo_query(entity: str, where_clause: Optional[str] = None, page_size: int = 1000):\n    realm_id = get_secret_value('qb_realmid')\n    url = f\"{qbo_base_url()}/v3/company/{realm_id}/query\"\n\n    start = 1\n    page_number = 1\n\n    # Limpia el where_clause por si viene None o espacios\n    where_clause = (where_clause or \"\").strip()\n\n    while True:\n        where_sql = f\" where {where_clause}\" if where_clause else \"\"\n        query = (\n            f\"select * from {entity}\"\n            f\"{where_sql} \"\n            f\"startposition {start} \"\n            f\"maxresults {page_size}\"\n        )\n\n        access_token = get_access_token()\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n\n        resp = None\n        for attempt in range(6):\n            resp = requests.post(url, headers=headers, data=query, timeout=60)\n\n            if resp.status_code in (429, 500, 502, 503, 504):\n                time.sleep(min(60, (2 ** attempt)) + random.random())\n                continue\n\n            resp.raise_for_status()\n            break\n\n        data = resp.json()\n\n        rows = (\n            data.get(\"QueryResponse\", {})\n            .get(entity, [])\n            or []\n        )\n\n        yield page_number, page_size, {\"query\": query}, rows\n\n        if len(rows) < page_size:\n            break\n\n        start += page_size\n        page_number += 1\n\n\n@data_loader\ndef invoices_extract(*args, **kwargs):\n    # En Mage, muchas veces las variables llegan dentro de kwargs[\"configuration\"]\n    cfg = kwargs.get(\"configuration\", {}) or {}\n\n    fecha_inicio = cfg.get(\"fecha_inicio\") or kwargs.get(\"fecha_inicio\")\n    fecha_fin = cfg.get(\"fecha_fin\") or kwargs.get(\"fecha_fin\")\n\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Faltan variables: fecha_inicio y fecha_fin (ej: 2024-01-01T00:00:00Z)\")\n\n    out = []\n\n    for w_start, w_end in iter_daily_windows(fecha_inicio, fecha_fin):\n        where = (\n            f\"MetaData.LastUpdatedTime >= '{iso_utc(w_start)}' \"\n            f\"and MetaData.LastUpdatedTime < '{iso_utc(w_end)}'\"\n        )\n\n        for page_number, page_size, request_payload, rows in qbo_query(\"Invoice\", None):\n            ingested_at = datetime.now(timezone.utc)\n\n            for r in rows:\n                invoice_id = str(r.get(\"Id\"))\n                if not invoice_id:\n                    continue\n\n                out.append({\n                    \"id\": invoice_id,\n                    \"payload\": r,\n                    \"ingested_at_utc\": ingested_at,\n                    \"extract_window_start_utc\": w_start,\n                    \"extract_window_end_utc\": w_end,\n                    \"page_number\": page_number,\n                    \"page_size\": page_size,\n                    \"request_payload\": request_payload,\n                })\n\n            print(\"TOTAL OUT:\", len(out))\n            return out\n\n\n", "file_path": "data_loaders/invoices_extract.py", "language": "python", "type": "data_loader", "uuid": "invoices_extract"}, "transformers/transform_qb_invoices.py:transformer:python:transform qb invoices": {"content": "import pandas as pd\n\ndef _safe_get(d, *keys, default=None):\n    cur = d\n    for k in keys:\n        if cur is None:\n            return default\n        if isinstance(cur, dict):\n            cur = cur.get(k, default)\n        else:\n            return default\n    return cur if cur is not None else default\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    data: lo que sale del bloque anterior (upsert_invoices_to_postgres)\n    En tu loader t\u00fa armaste \"out.append({ id, payload, ... })\"\n    As\u00ed que aqu\u00ed data deber\u00eda ser una lista de dicts.\n    \"\"\"\n    if data is None:\n        return pd.DataFrame()\n\n    # Si Mage lo manda como df ya, lo convertimos a records\n    if isinstance(data, pd.DataFrame):\n        rows = data.to_dict(\"records\")\n    else:\n        rows = data\n\n    invoice_rows = []\n    line_rows = []\n\n    for r in rows:\n        payload = r.get(\"payload\") or {}\n        inv_id = r.get(\"id\") or _safe_get(payload, \"Id\")\n\n        # Campos t\u00edpicos de QBO Invoice\n        invoice_rows.append({\n            \"id\": inv_id,\n            \"doc_number\": _safe_get(payload, \"DocNumber\"),\n            \"txn_date\": _safe_get(payload, \"TxnDate\"),\n            \"currency\": _safe_get(payload, \"CurrencyRef\", \"value\"),\n            \"customer_id\": _safe_get(payload, \"CustomerRef\", \"value\"),\n            \"customer_name\": _safe_get(payload, \"CustomerRef\", \"name\"),\n            \"total_amt\": _safe_get(payload, \"TotalAmt\"),\n            \"balance\": _safe_get(payload, \"Balance\"),\n            \"status\": _safe_get(payload, \"TxnStatus\"),\n            \"private_note\": _safe_get(payload, \"PrivateNote\"),\n            \"create_time\": _safe_get(payload, \"MetaData\", \"CreateTime\"),\n            \"last_updated_time\": _safe_get(payload, \"MetaData\", \"LastUpdatedTime\"),\n            \"sync_token\": _safe_get(payload, \"SyncToken\"),\n\n            # metadata del extract\n            \"ingested_at_utc\": r.get(\"ingested_at_utc\"),\n            \"extract_window_start_utc\": r.get(\"extract_window_start_utc\"),\n            \"extract_window_end_utc\": r.get(\"extract_window_end_utc\"),\n            \"page_number\": r.get(\"page_number\"),\n            \"page_size\": r.get(\"page_size\"),\n        })\n\n        # Lines (si luego quieres explotar)\n        lines = payload.get(\"Line\") or []\n        for i, ln in enumerate(lines):\n            line_rows.append({\n                \"invoice_id\": inv_id,\n                \"line_idx\": i,\n                \"line_id\": ln.get(\"Id\"),\n                \"detail_type\": ln.get(\"DetailType\"),\n                \"amount\": ln.get(\"Amount\"),\n                \"description\": ln.get(\"Description\"),\n                \"item_id\": _safe_get(ln, \"SalesItemLineDetail\", \"ItemRef\", \"value\"),\n                \"item_name\": _safe_get(ln, \"SalesItemLineDetail\", \"ItemRef\", \"name\"),\n                \"qty\": _safe_get(ln, \"SalesItemLineDetail\", \"Qty\"),\n                \"unit_price\": _safe_get(ln, \"SalesItemLineDetail\", \"UnitPrice\"),\n            })\n\n    invoices_df = pd.DataFrame(invoice_rows)\n    lines_df = pd.DataFrame(line_rows)\n\n    # Devuelve ambos para que luego puedas exportar 2 tablas si quieres\n    return {\n        \"invoices\": invoices_df,\n        \"lines\": lines_df,\n    }\n", "file_path": "transformers/transform_qb_invoices.py", "language": "python", "type": "transformer", "uuid": "transform_qb_invoices"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    fecha_fin: '2024-01-03T00:00:00Z'\n    fecha_inicio: '2024-01-01T00:00:00Z'\n  downstream_blocks:\n  - transform_qb_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: invoices_extract\n  retry_config: {}\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: invoices_extract\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: upsert_invoices_to_postgres\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_invoices\n  uuid: upsert_invoices_to_postgres\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - upsert_invoices_to_postgres\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - invoices_extract\n  uuid: transform_qb_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-01 22:53:03.576961+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "/home/src/scheduler/data_loaders/invoices_extract.py:data_loader:python:home/src/scheduler/data loaders/invoices extract": {"content": "import base64\nimport random\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Optional\n\n\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\ndef iso_utc(dt: datetime) -> str:\n    return dt.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n\n\ndef parse_iso_z(s: str) -> datetime:\n    return datetime.fromisoformat(s.replace('Z', '+00:00')).astimezone(timezone.utc)\n\n\ndef iter_daily_windows(start_iso: str, end_iso: str):\n    start = parse_iso_z(start_iso)\n    end = parse_iso_z(end_iso)\n    cur = start\n    while cur < end:\n        nxt = min(cur + timedelta(days=1), end)\n        yield cur, nxt\n        cur = nxt\n\n\ndef get_access_token() -> str:\n    client_id = get_secret_value('qb_clientid')\n    client_secret = get_secret_value('qb_clientsecret')\n    refresh_token = get_secret_value('qb_refreshtoken')\n\n    basic = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n    headers = {\n        \"Authorization\": f\"Basic {basic}\",\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    data = {\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token}\n\n    r = requests.post(\n        \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\",\n        headers=headers,\n        data=data,\n        timeout=30,\n    )\n    r.raise_for_status()\n    return r.json()[\"access_token\"]\n\n\ndef qbo_base_url() -> str:\n    return \"https://sandbox-quickbooks.api.intuit.com\"\n\n\ndef qbo_query(entity: str, where_clause: Optional[str] = None, page_size: int = 1000):\n    realm_id = get_secret_value('qb_realmid')\n    url = f\"{qbo_base_url()}/v3/company/{realm_id}/query\"\n\n    start = 1\n    page_number = 1\n\n    where_clause = (where_clause or \"\").strip()\n\n    while True:\n        where_sql = f\" where {where_clause}\" if where_clause else \"\"\n        query = (\n            f\"select * from {entity}\"\n            f\"{where_sql} \"\n            f\"startposition {start} \"\n            f\"maxresults {page_size}\"\n        )\n\n        access_token = get_access_token()\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/text\",\n        }\n\n        resp = None\n        for attempt in range(6):\n            resp = requests.post(url, headers=headers, data=query, timeout=60)\n\n            if resp.status_code in (429, 500, 502, 503, 504):\n                time.sleep(min(60, (2 ** attempt)) + random.random())\n                continue\n\n            resp.raise_for_status()\n            break\n\n        data = resp.json()\n\n        rows = (\n            data.get(\"QueryResponse\", {})\n            .get(entity, [])\n            or []\n        )\n\n        yield page_number, page_size, {\"query\": query}, rows\n\n        if len(rows) < page_size:\n            break\n\n        start += page_size\n        page_number += 1\n\n\n@data_loader\ndef invoices_extract(*args, **kwargs):\n\n    cfg = kwargs.get(\"configuration\", {}) or {}\n\n    fecha_inicio = cfg.get(\"fecha_inicio\") or kwargs.get(\"fecha_inicio\")\n    fecha_fin = cfg.get(\"fecha_fin\") or kwargs.get(\"fecha_fin\")\n\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Faltan variables: fecha_inicio y fecha_fin (ej: 2024-01-01T00:00:00Z)\")\n\n    out = []\n\n    for w_start, w_end in iter_daily_windows(fecha_inicio, fecha_fin):\n        where = (\n            f\"MetaData.LastUpdatedTime >= '{iso_utc(w_start)}' \"\n            f\"and MetaData.LastUpdatedTime < '{iso_utc(w_end)}'\"\n        )\n\n        for page_number, page_size, request_payload, rows in qbo_query(\"Invoice\", None):\n            ingested_at = datetime.now(timezone.utc)\n\n            for r in rows:\n                invoice_id = str(r.get(\"Id\"))\n                if not invoice_id:\n                    continue\n\n                out.append({\n                    \"id\": invoice_id,\n                    \"payload\": r,\n                    \"ingested_at_utc\": ingested_at,\n                    \"extract_window_start_utc\": w_start,\n                    \"extract_window_end_utc\": w_end,\n                    \"page_number\": page_number,\n                    \"page_size\": page_size,\n                    \"request_payload\": request_payload,\n                })\n\n            print(\"TOTAL OUT:\", len(out))\n            return out\n\n\n", "file_path": "/home/src/scheduler/data_loaders/invoices_extract.py", "language": "python", "type": "data_loader", "uuid": "invoices_extract"}, "/home/src/scheduler/data_exporters/upsert_invoices_to_postgres.py:data_exporter:python:home/src/scheduler/data exporters/upsert invoices to postgres": {"content": "import json\nfrom datetime import timezone\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef upsert_invoices_to_postgres(data, *args, **kwargs):\n    \"\"\"\n    Recibe output del transformer:\n      data = {\"invoices\": invoices_df, \"lines\": lines_df}\n    y hace upsert solo a raw.qb_invoices.\n    \"\"\"\n\n    if not data or \"invoices\" not in data or data[\"invoices\"] is None:\n        return {\"inserted_or_updated\": 0, \"note\": \"No invoices data returned from transformer.\"}\n\n    invoices_df = data[\"invoices\"]\n\n    if hasattr(invoices_df, \"empty\") and invoices_df.empty:\n        return {\"inserted_or_updated\": 0, \"note\": \"Invoices dataframe is empty.\"}\n\n    records = invoices_df.to_dict(\"records\")\n\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    profile = \"default\"\n\n    rows = []\n    for r in records:\n        payload = r.get(\"payload\")\n        request_payload = r.get(\"request_payload\")\n\n        def to_utc_iso(x):\n            if x is None:\n                return None\n            if isinstance(x, str):\n                return x\n            return x.astimezone(timezone.utc).isoformat()\n\n        rows.append({\n            \"id\": str(r.get(\"id\")),\n            \"payload\": json.dumps(payload) if isinstance(payload, (dict, list)) else (payload or \"{}\"),\n            \"ingested_at_utc\": to_utc_iso(r.get(\"ingested_at_utc\")),\n            \"extract_window_start_utc\": to_utc_iso(r.get(\"extract_window_start_utc\")),\n            \"extract_window_end_utc\": to_utc_iso(r.get(\"extract_window_end_utc\")),\n            \"page_number\": int(r.get(\"page_number\") or 0),\n            \"page_size\": int(r.get(\"page_size\") or 0),\n            \"request_payload\": json.dumps(request_payload) if isinstance(request_payload, (dict, list)) else (request_payload or \"{}\"),\n        })\n\n    sql = \"\"\"\n    INSERT INTO raw.qb_invoices (\n      id, payload, ingested_at_utc, extract_window_start_utc, extract_window_end_utc,\n      page_number, page_size, request_payload\n    )\n    VALUES (\n      %(id)s, %(payload)s::jsonb, %(ingested_at_utc)s::timestamptz,\n      %(extract_window_start_utc)s::timestamptz, %(extract_window_end_utc)s::timestamptz,\n      %(page_number)s, %(page_size)s, %(request_payload)s::jsonb\n    )\n    ON CONFLICT (id) DO UPDATE SET\n      payload = EXCLUDED.payload,\n      ingested_at_utc = EXCLUDED.ingested_at_utc,\n      extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n      extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n      page_number = EXCLUDED.page_number,\n      page_size = EXCLUDED.page_size,\n      request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    with Postgres.with_config(ConfigFileLoader(config_path, profile)) as loader:\n        for row in rows:\n            loader.execute(sql, **row)\n\n        count = loader.load(\"SELECT COUNT(*) AS n FROM raw.qb_invoices;\")\n\n    return {\"inserted_or_updated\": len(rows), \"raw.qb_invoices_count\": count}\n", "file_path": "/home/src/scheduler/data_exporters/upsert_invoices_to_postgres.py", "language": "python", "type": "data_exporter", "uuid": "upsert_invoices_to_postgres"}, "/home/src/scheduler/transformers/transform_qb_invoices.py:transformer:python:home/src/scheduler/transformers/transform qb invoices": {"content": "import pandas as pd\n\ndef _safe_get(d, *keys, default=None):\n    cur = d\n    for k in keys:\n        if cur is None:\n            return default\n        if isinstance(cur, dict):\n            cur = cur.get(k, default)\n        else:\n            return default\n    return cur if cur is not None else default\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    data: lo que sale del bloque anterior (upsert_invoices_to_postgres)\n    En tu loader t\u00fa armaste \"out.append({ id, payload, ... })\"\n    As\u00ed que aqu\u00ed data deber\u00eda ser una lista de dicts.\n    \"\"\"\n    if data is None:\n        return pd.DataFrame()\n\n    if isinstance(data, pd.DataFrame):\n        rows = data.to_dict(\"records\")\n    else:\n        rows = data\n\n    invoice_rows = []\n    line_rows = []\n\n    for r in rows:\n        payload = r.get(\"payload\") or {}\n        inv_id = r.get(\"id\") or _safe_get(payload, \"Id\")\n\n        invoice_rows.append({\n            \"id\": inv_id,\n            \"doc_number\": _safe_get(payload, \"DocNumber\"),\n            \"txn_date\": _safe_get(payload, \"TxnDate\"),\n            \"currency\": _safe_get(payload, \"CurrencyRef\", \"value\"),\n            \"customer_id\": _safe_get(payload, \"CustomerRef\", \"value\"),\n            \"customer_name\": _safe_get(payload, \"CustomerRef\", \"name\"),\n            \"total_amt\": _safe_get(payload, \"TotalAmt\"),\n            \"balance\": _safe_get(payload, \"Balance\"),\n            \"status\": _safe_get(payload, \"TxnStatus\"),\n            \"private_note\": _safe_get(payload, \"PrivateNote\"),\n            \"create_time\": _safe_get(payload, \"MetaData\", \"CreateTime\"),\n            \"last_updated_time\": _safe_get(payload, \"MetaData\", \"LastUpdatedTime\"),\n            \"sync_token\": _safe_get(payload, \"SyncToken\"),\n\n            \"ingested_at_utc\": r.get(\"ingested_at_utc\"),\n            \"extract_window_start_utc\": r.get(\"extract_window_start_utc\"),\n            \"extract_window_end_utc\": r.get(\"extract_window_end_utc\"),\n            \"page_number\": r.get(\"page_number\"),\n            \"page_size\": r.get(\"page_size\"),\n        })\n\n        lines = payload.get(\"Line\") or []\n        for i, ln in enumerate(lines):\n            line_rows.append({\n                \"invoice_id\": inv_id,\n                \"line_idx\": i,\n                \"line_id\": ln.get(\"Id\"),\n                \"detail_type\": ln.get(\"DetailType\"),\n                \"amount\": ln.get(\"Amount\"),\n                \"description\": ln.get(\"Description\"),\n                \"item_id\": _safe_get(ln, \"SalesItemLineDetail\", \"ItemRef\", \"value\"),\n                \"item_name\": _safe_get(ln, \"SalesItemLineDetail\", \"ItemRef\", \"name\"),\n                \"qty\": _safe_get(ln, \"SalesItemLineDetail\", \"Qty\"),\n                \"unit_price\": _safe_get(ln, \"SalesItemLineDetail\", \"UnitPrice\"),\n            })\n\n    invoices_df = pd.DataFrame(invoice_rows)\n    lines_df = pd.DataFrame(line_rows)\n\n    return {\n        \"invoices\": invoices_df,\n        \"lines\": lines_df,\n    }\n", "file_path": "/home/src/scheduler/transformers/transform_qb_invoices.py", "language": "python", "type": "transformer", "uuid": "transform_qb_invoices"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}